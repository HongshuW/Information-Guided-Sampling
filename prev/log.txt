Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 4?
Iterations: 5
Do we sample when decoding? False
All outputs:
{"string1": 5, "string2": 3, "string3": 4}Classified outputs:
{"string1": 5, "string2": 3, "string3": 4}Idea distribution:
{"string1": 4, "string2": 4, "string3": 4}
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 4?
Iterations: 5
Do we sample when decoding? False
All outputs:
{"string1": 5, "string2": 3, "string3": 4}Classified outputs:
{"string1": 5, "string2": 3, "string3": 4}Idea distribution:
{"string1": 4, "string2": 4, "string3": 4}
2024-02-02 02:40:40,198:INFO:Experiment record time: 2024-02-02 02:40:40
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Iterations: 3
Do we sample when decoding? True
Top_p: 0.9
Top_k: 5
Temperature: 1.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 3}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 3}
Idea distribution:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 0}
2024-02-02 04:02:07,217:INFO:Experiment record time: 2024-02-02 04:02:07
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Iterations: 100
Do we sample when decoding? True
Top_p: 0.9
Top_k: 1000
Temperature: 0.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 05:41:47,670:INFO:Experiment record time: 2024-02-02 05:41:47
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Iterations: 100
Do we sample when decoding? True
Top_p: 0.8
Top_k: 1000
Temperature: 0.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 06:16:19,934:INFO:Experiment record time: 2024-02-02 06:16:19
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.9
Top_k: 1000
Temperature: 1.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 07:24:02,124:INFO:Experiment record time: 2024-02-02 07:24:02
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.7
Top_k: 1000
Temperature: 0.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 08:02:31,213:INFO:Experiment record time: 2024-02-02 08:02:31
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.8
Top_k: 1000
Temperature: 1.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 09:17:57,340:INFO:Experiment record time: 2024-02-02 09:17:57
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.95
Top_k: 1000
Temperature: 0.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 09:48:07,918:INFO:Experiment record time: 2024-02-02 09:48:07
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.7
Top_k: 1000
Temperature: 1.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 11:14:33,503:INFO:Experiment record time: 2024-02-02 11:14:33
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.9
Top_k: 1000
Temperature: 1.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 11:42:26,215:INFO:Experiment record time: 2024-02-02 11:42:26
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.95
Top_k: 1000
Temperature: 1.5
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 12:40:30,418:INFO:Experiment record time: 2024-02-02 12:40:30
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.8
Top_k: 1000
Temperature: 1.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 13:44:32,976:INFO:Experiment record time: 2024-02-02 13:44:32
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.9
Top_k: 1000
Temperature: 2.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 14:52:19,049:INFO:Experiment record time: 2024-02-02 14:52:19
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.7
Top_k: 1000
Temperature: 1.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 15:41:49,380:INFO:Experiment record time: 2024-02-02 15:41:49
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.8
Top_k: 1000
Temperature: 2.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
2024-02-02 16:55:26,834:INFO:Experiment record time: 2024-02-02 16:55:26
Model: meta-llama/Llama-2-7b-hf
Prompt: Randomly generate a binary string of length at most 2?
Grammar: string_01.ebnf
Iterations: 100
Do we sample when decoding? True
Top_p: 0.95
Top_k: 1000
Temperature: 1.0
All outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "1000000000000000000000000000000000000000": 100}
Classified outputs:
{"0": 0, "1": 0, "00": 0, "01": 0, "10": 0, "11": 0, "other": 100}
Idea distribution:
{"0": 17, "1": 17, "00": 17, "01": 17, "10": 17, "11": 17, "other": 0}
